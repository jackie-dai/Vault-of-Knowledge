

## Numerical Differentiation
Advantage
- Can apply to any error function 
- Easy to implement
Disadvantage
- Expensive
- Approximate

## Automatic Differentiation


## Backward Propagation

### Adjoint variable

1(e^v_3) + 1(1)


## Optimizers

### Gradient Descent
![[Pasted image 20251109014222.png]]
Oscillates in the vertical d

### Adaptative Gradient Descent (AdaGrad)